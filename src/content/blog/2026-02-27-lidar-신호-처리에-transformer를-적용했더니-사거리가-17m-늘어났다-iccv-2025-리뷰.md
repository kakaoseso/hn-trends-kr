---
title: "Lidar 신호 처리에 Transformer를 적용했더니 사거리가 17m 늘어났다 (ICCV 2025 리뷰)"
description: "Lidar 신호 처리에 Transformer를 적용했더니 사거리가 17m 늘어났다 (ICCV 2025 리뷰)"
pubDate: "2026-02-27T01:46:33Z"
---

자율주행 업계에서 일하다 보면 **Lidar(라이다)** 센서를 '진실의 입(Ground Truth)'처럼 여기는 경향이 있습니다. 카메라가 "저거 사람 같은데?"라고 의심할 때, 라이다는 "전방 32.5m에 물체 있음"이라고 단정 지어주니까요.

하지만 하드웨어 레벨로 조금만 깊게 들어가면, 라이다가 뱉어내는 Point Cloud(점군) 데이터 역시 수많은 **후처리(Post-processing)** 의 결과물일 뿐이라는 걸 알게 됩니다. 레이저를 쏘고, 반사되어 돌아오는 아날로그 파형(Waveform)을 디지털로 바꾸고, 거기서 '피크(Peak)'를 찾아내 좌표로 변환하는 과정이 필요하니까요.

오늘 소개할 논문은 바로 이 **'신호 처리(DSP)'** 단계에 딥러닝, 그것도 **Transformer** 를 끼얹은 흥미로운 연구입니다. ICCV 2025에 발표된 **"Lidar Waveforms are Worth 40x128x33 Words"** 를 엔지니어 관점에서 뜯어보겠습니다.

## 기존 방식의 한계: 피크를 찾아라

전통적인 라이다 DSP는 단순합니다. 돌아오는 파형에서 임계값(Threshold)을 넘는 피크가 있으면 "여기 물체가 있다"고 판단합니다. 문제는 **악천후(안개, 비)** 나 **장거리** 상황입니다.

신호 대 잡음비(SNR)가 낮아지면 진짜 신호가 노이즈에 묻혀버립니다. 기존 DSP는 이 노이즈 속에서 피크를 못 찾거나, 노이즈를 물체로 착각해 고스트(Artifact)를 만들어냅니다. 이걸 해결하려고 보통은 더 강한 레이저를 쓰거나 센서 크기를 키우는데, 비용과 전력 소모가 만만치 않죠.

## 제안된 방법: Raw Waveform + Transformer

이 논문의 저자들은 아주 직관적인 질문을 던집니다.

> "왜 파형 하나만 보고 판단해? 옆 픽셀 파형도 같이 보면 되잖아?"

사람이 안개 속에서 물체를 볼 때, 점 하나만 보고 판단하지 않습니다. 주변 맥락(Context)을 보죠. 이 연구는 라이다의 **Raw Waveform** 자체를 Transformer의 입력으로 사용합니다.

1.  **입력:** 개별 파형 하나만 보는 게 아니라, 인접한 파형들의 정보를 함께 봅니다. (Spatial Context)
2.  **처리:** Vision Transformer(ViT) 구조를 변형해 파형 데이터를 패치 단위로 처리합니다. 제목의 '40x128x33 Words'는 바로 이 입력 텐서의 차원과 패치 구성을 의미합니다.
3.  **출력:** 단순한 피크 검출이 아니라, 학습된 모델이 고해상도의 Multi-echo Point Cloud를 생성합니다.

### 결과가 꽤 놀랍습니다

논문에 따르면, 기존 피크 검출 방식(Peak Finding) 대비 **Chamfer Distance(오차)** 를 32cm나 줄였습니다. 더 인상적인 건 사거리(Range)입니다.

- **안개 상황:** 최대 인식 거리가 **17m 증가**
- **일반 상황:** 최대 인식 거리가 **14m 증가**

자율주행에서 17m는 고속 주행 시 제동 거리를 확보하느냐 마느냐를 가르는 엄청난 차이입니다. 하드웨어 스펙 변경 없이 소프트웨어(DSP) 변경만으로 이런 이득을 얻을 수 있다는 건 꽤 매력적입니다.

## Principal Engineer의 시선: 현실성은 있을까?

솔직히 처음 제목을 봤을 때는 "또 무거운 Transformer를 엣지(Edge)에 넣으라고?"라며 회의적이었습니다. 하지만 내용을 뜯어보니 방향성은 확실히 맞습니다.

### 1. Perception의 이동 (Sensor -> DSP)
과거 컴퓨터 비전이 '엣지 검출(Canny Edge)' 같은 수작업 알고리즘에서 CNN/Transformer 기반의 End-to-End 학습으로 넘어갔듯이, 센서 신호 처리도 **Learned DSP** 로 넘어가는 건 시간문제입니다. Raw Data를 버리지 않고 최대한 활용하는 것이 정보 이론적으로도 우월하니까요.

### 2. 문제는 역시 Compute Cost
라이다는 초당 수십만에서 수백만 개의 펄스를 쏩니다. 이 모든 파형(Waveform)에 대해 Transformer를 돌린다? 현재의 차량용 ECU나 FPGA로는 **Latency** 감당이 쉽지 않을 겁니다. 논문에서도 실시간성보다는 성능 향상에 초점을 맞추고 있습니다. 하지만 전용 NPU가 센서 모듈 안에 내장된다면 불가능한 이야기도 아닙니다.

## Hacker News 반응과 생각

해커뉴스(Hacker News)에서도 재미있는 토론들이 오갔습니다.

- **간섭(Interference) 문제:** 한 유저가 "교차로에 자율주행차 100대가 모이면 서로 라이다 신호를 쏘느라 엉망이 되지 않나?"라고 물었습니다.
    - 사실 이건 업계에서는 이미 **Crosstalk** 방지 기술(Pulse coding, Phase shifting 등)로 어느 정도 해결된 문제입니다. 하지만 이 논문처럼 주변 파형의 맥락을 보는 방식이 도입되면, 다른 차의 신호를 노이즈로 걸러내는(Denoising) 성능도 덩달아 좋아질 수 있습니다. 반대로, 학습 데이터에 없는 패턴의 간섭이 들어오면 모델이 환각(Hallucination)을 일으킬 위험도 배제할 순 없겠죠.

- **취미용 라이다:** "취미로 쓸 만한 라이다가 있냐?"는 질문에 $20짜리 로봇 청소기 부품부터 Livox 같은 준전문가용 제품 추천이 줄을 이었습니다. 확실히 라이다의 진입 장벽이 낮아지고 있습니다.

## 결론: Raw Data is King

이 논문은 **"센서가 주는 1차 가공 데이터를 믿지 마라, 원본(Raw)을 다시 봐라"** 라는 메시지를 줍니다. 

당장 상용차에 적용하기엔 연산 비용이라는 거대한 벽이 있지만, 장기적으로 고성능 자율주행 센서 파이프라인은 결국 이런 **Data-Driven DSP** 형태로 진화할 것입니다. 하드웨어의 물리적 한계를 소프트웨어(AI)로 돌파하는 전형적이고 훌륭한 사례입니다.

**참고 자료:**
- [ICCV 2025 논문 원문](https://openaccess.thecvf.com/content/ICCV2025/html/Scheuble_Lidar_Waveforms_are_Worth_40x128x33_Words_ICCV_2025_paper.html)
- [Hacker News 토론](https://news.ycombinator.com/item?id=47121352)
