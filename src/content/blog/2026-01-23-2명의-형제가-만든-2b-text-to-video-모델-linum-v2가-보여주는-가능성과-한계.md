---
title: "2명의 형제가 만든 2B Text-to-Video 모델: Linum v2가 보여주는 가능성과 한계"
description: "2명의 형제가 만든 2B Text-to-Video 모델: Linum v2가 보여주는 가능성과 한계"
pubDate: "2026-01-23T05:10:02Z"
---

Sora나 Runway Gen-3 같은 모델들이 세상을 놀라게 하고 있지만, 엔지니어 입장에서 가장 답답한 건 '내 로컬 머신에서 돌릴 수 없다는 것'입니다. API 뒤에 숨겨진 블랙박스는 비즈니스적으로는 이해가 가지만, 기술적인 갈증을 해소해주진 못하니까요.

그런 와중에 Hacker News에서 꽤 흥미로운 프로젝트를 발견했습니다. **Linum v2** 라는 Text-to-Video 모델인데, 배경 스토리가 아주 낭만적입니다. 거대 테크 기업의 연구소가 아니라, **두 명의 형제가 2년 동안 바닥부터(from scratch) 만들었다** 고 합니다. 게다가 **Apache 2.0** 라이선스입니다. 이건 못 참죠.

오늘은 이 모델의 기술적 특징과 커뮤니티에서 지적된 VRAM 이슈, 그리고 제가 바라보는 현시점의 한계에 대해 깊게 파고들어 보겠습니다.

## 2B 파라미터의 함정: 왜 가볍지 않을까?

Linum v2는 20억(2B) 파라미터 모델입니다. LLM 기준으로 생각하면 '초경량' 모델에 속합니다. 요즘 엣지 디바이스용 LLM도 7B, 8B 정도 하니까요. 그래서 처음 스펙을 봤을 때는 "오, 이거 소비자용 GPU에서도 쌩쌩 돌아가겠는데?"라고 생각했습니다.

하지만 HN 댓글 창과 실제 구동 후기를 보면 현실은 달랐습니다. 360p 영상을 생성하는 데도 **20GB 이상의 VRAM** 이 필요하다는 리포트가 쏟아졌습니다. 2B 모델이 왜 이렇게 무거울까요? 여기에 엔지니어링의 디테일이 숨어 있습니다.

### 범인은 T5 Encoder

모델 구조를 뜯어보면 흥미로운 비대칭성이 보입니다. 비디오 생성 모델 자체는 2B지만, 텍스트를 이해하는 인코더로 **T5-XXL** 을 사용하고 있습니다. 이 녀석 혼자 파라미터가 약 5B(50억) 개입니다. 배보다 배꼽이 더 큰 상황이죠.

- **T5 Encoder:** ~10GB VRAM (bfloat16 기준)
- **Video Context:** 720p 비디오 프레임들을 처리하기 위한 거대한 컨텍스트 윈도우

결국 모델 가중치(Weights)보다, 텍스트 인코딩과 비디오 생성을 위한 Activation 메모리가 VRAM을 다 잡아먹고 있는 구조입니다. 2B라는 숫자만 보고 접근했던 유저들이 `CUDA Out of Memory`를 보고 당황할 수밖에 없는 이유입니다.

## 현장의 목소리와 최적화 전략

Hacker News 스레드에서는 이 메모리 병목에 대한 논의가 활발했습니다. 개발자(형제 중 한 명으로 추정되는)와 유저들이 나눈 대화에서 몇 가지 핵심적인 최적화 포인트가 나왔습니다.

제가 Principal Engineer로서 팀원들에게 항상 강조하는 것이 **"리소스 라이프사이클 관리"** 인데, 여기서도 정확히 그 문제가 드러납니다.

### 1. T5 Encoder 조기 해제 (Early Release)

텍스트 프롬프트를 벡터로 변환(Encoding)하고 나면, 사실 T5는 더 이상 필요가 없습니다. 하지만 많은 구현체들이 추론(Inference) 파이프라인이 끝날 때까지 인코더를 메모리에 쥐고 있습니다. 이를 코드로 해결하면 수 기가바이트의 VRAM을 즉시 확보할 수 있습니다.

### 2. Manual Layer Offloading

이건 LLM을 로컬에서 돌릴 때 `llama.cpp` 같은 곳에서 자주 쓰는 기법입니다. 계산이 끝난 레이어는 CPU 램으로 내리고(Offload), 필요한 레이어만 GPU에 올리는 방식입니다. 속도(Latency)는 좀 희생하더라도, 24GB VRAM을 가진 RTX 3090/4090 유저들이 720p 영상을 뽑아내려면 필수적인 선택입니다.

## 개인적인 평가: 아직은 '장난감'과 '도구' 사이

솔직히 말해서, 퀄리티 측면에서 Linum v2가 Sora나 Kling을 위협할 수준은 아닙니다. 생성된 영상은 2~5초 길이에 불과하고, 360p/720p 해상도는 요즘 기준엔 다소 아쉽습니다. 움직임의 일관성(Consistency)도 아직 갈 길이 멉니다.

하지만 이 프로젝트가 가진 의미는 **"가능성의 증명"** 에 있습니다.

1.  **데이터셋의 중요성:** 이 형제들은 단순히 모델만 깎은 게 아니라, 데이터셋 큐레이션에 엄청난 공을 들였을 겁니다. From scratch 학습은 결국 데이터 싸움이니까요.
2.  **오픈소스 비디오 생성의 민주화:** Stable Diffusion이 이미지 생성의 빗장을 풀었듯, 누군가는 비디오 쪽에서 그 역할을 해야 합니다. Linum이 그 시발점이 될 수 있습니다.

## 결론 (Verdict)

현업 프로젝트에 당장 투입할 수 있냐고 묻는다면, 제 대답은 **"No"** 입니다. VRAM 요구량 대비 아웃풋 퀄리티(ROI)가 아직 맞지 않습니다.

하지만 **연구용, 혹은 토이 프로젝트용** 으로는 강력 추천합니다. 특히 2B 사이즈의 모델이 비디오의 시공간적 특징을 어떻게 학습했는지 뜯어보고 싶다면, 이보다 좋은 교보재는 없을 겁니다. T5 인코더를 더 가벼운 모델(CLIP이나 작은 LLM)로 교체하거나, 양자화(Quantization)를 적용해 보는 것도 좋은 사이드 프로젝트 주제가 될 것 같네요.

**참고 링크:**
- [Hugging Face Collection](https://huggingface.co/collections/Linum-AI/linum-v2-2b-text-to-video)
- [Hacker News Discussion](https://news.ycombinator.com/item?id=46721488)
