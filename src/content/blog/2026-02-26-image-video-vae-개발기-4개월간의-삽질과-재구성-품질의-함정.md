---
title: "Image-Video VAE 개발기: 4개월간의 삽질과 재구성 품질의 함정"
description: "Image-Video VAE 개발기: 4개월간의 삽질과 재구성 품질의 함정"
pubDate: "2026-02-26T01:16:02Z"
---

엔지니어링에서 가장 고통스러운 순간은 언제일까요? 밤샘 디버깅? 프로덕션 장애? 아닙니다. 바로 **"우리가 최적화하던 지표가 사실은 쓸모없었다"** 는 것을 깨닫는 순간입니다.

최근 Linum 팀이 공개한 4개월간의 Image-Video VAE 개발 포스트모템은 바로 이 지점을 통렬하게 보여줍니다. 이들은 4개월 동안 NaN(Not a Number) 에러와 싸우고, 미스터리한 얼룩(Splotches)을 지우기 위해 별짓을 다 했지만, 결국 **"더 좋은 압축(Reconstruction)이 더 좋은 생성(Generation)을 보장하지 않는다"** 는 결론에 도달했습니다.

오늘은 이들의 삽질 기록을 통해, 멀티모달 VAE 학습의 기술적 난제들과 우리가 흔히 빠지는 'Metric의 함정'에 대해 깊이 파고들어 보려 합니다.

## 왜 굳이 VAE를 직접 깎았을까?

생성형 비디오 모델의 핵심은 Diffusion Transformer입니다. 하지만 Transformer의 고질병인 $O(N^2)$ Attention 비용 때문에 픽셀 공간(Pixel Space)에서 직접 연산하는 건 불가능에 가깝습니다. 720p 5초짜리 영상(24fps)을 토큰화하면 약 1억 1천만 개의 토큰이 나옵니다. H100 수십 장을 써도 감당이 안 되죠.

그래서 우리는 VAE(Variational Autoencoder)를 써서 이 데이터를 압축된 Latent Space로 밀어 넣습니다. Linum 팀은 2024년 7월부터 11월까지 자체 Image-Video VAE를 학습시켰습니다. 목표는 단순했습니다. **"최대한 원본과 똑같이 복원해내는 것."**

## 기술적 난관 1: 이미지와 비디오의 불편한 동거

이미지와 비디오를 동시에 학습(Co-training)시키는 건 생각보다 훨씬 까다롭습니다. Linum은 처음에 단일 이미지를 4프레임짜리 "정적 비디오(Static Video)"로 패딩해서 넣었는데, 결과물이 처참했습니다.



럭비 선수의 얼굴이 뭉개진 게 보이시나요? 원인을 파헤쳐보니 **Loss Function의 스케일링 문제** 였습니다.

보통 Loss는 모든 차원에 대해 합산(Sum)하는데, 비디오 데이터 텐서가 이미지보다 압도적으로 큽니다(약 10배). 이러니 Optimizer 입장에서는 이미지는 무시하고 비디오 Loss만 줄이는 게 이득인 상황이 된 거죠. 그렇다고 평균(Mean)을 쓰자니, 작은 이미지 텐서의 픽셀 하나가 갖는 Gradient 영향력이 너무 커져서 학습이 튀어버립니다.

결국 이들은 고정된 참조 형상(Reference Shape)을 기준으로 정규화하는 방식을 택했습니다. 이 대목에서 저는 무릎을 탁 쳤습니다. 멀티모달 학습에서 Modality 간의 밸런싱은 언제나 골치 아픈 문제인데, 단순히 가중치를 조절하는 게 아니라 텐서 크기에 따른 근본적인 Gradient 설계를 다시 해야 한다는 좋은 사례입니다.

## 기술적 난관 2: 유령 같은 얼룩(Splotches)과 Normalization의 전쟁

학습 중간부터 영상 구석에 정체불명의 녹색, 검은색 얼룩이 생기기 시작했습니다. 이건 저도 VAE 학습할 때 종종 겪었던 문제입니다.



원인은 **Group Norm** 이었습니다. Group Norm은 그룹 내의 특정 채널 값이 튀면, 나머지 채널들의 신호를 억눌러버리는 경향이 있습니다. 이 때문에 특정 픽셀에 정보가 과도하게 집중되면서 얼룩이 생기는 거죠.

Linum은 이를 해결하기 위해 **SMC(Self-Modulating Convolution)** 를 도입하고, Attention Block의 Group Norm을 Pixel Norm으로 교체했습니다. Meta의 MovieGen 논문에서도 비슷한 문제를 겪고 Outlier Penalty를 추가했다고 하는데, 결국 딥러닝에서 Normalization 레이어의 선택은 단순한 취향 차이가 아니라 모델의 생사 여부를 가르는 결정적 요소입니다.

## 반전: "그냥 Wan 2.1 씁시다"

이렇게 4개월을 고생해서 꽤 괜찮은 VAE를 만들었습니다. 그런데 결론이 뭐냐고요? **그냥 Wan 2.1의 VAE를 쓰기로 했습니다.**

허무하신가요? 하지만 이게 시니어 엔지니어의 판단입니다. Wan 2.1의 VAE가 성능은 비슷한데, Full Spatio-temporal Attention을 쓰지 않아 임베딩 속도가 훨씬 빠르고 쌌기 때문입니다. 자체 기술 확보도 중요하지만, 비즈니스 임팩트와 비용 효율성을 따졌을 때 'Build'보다 'Buy(or Adapt)'가 낫다는 냉정한 판단을 내린 거죠.

## 핵심 교훈: 재구성 품질(Reconstruction)의 배신

이 글의 하이라이트는 여기입니다. Linum 팀은 **"재구성을 완벽하게 하려다 오히려 모델을 망쳤다"** 고 고백합니다.

데이터셋에는 저화질 JPEG 이미지들이 섞여 있습니다. VAE가 이 이미지들을 픽셀 단위로 완벽하게 복원하려고 노력한다는 건, 사실상 **JPEG 압축 노이즈(Artifact)를 학습한다는 뜻** 과 같습니다.



이렇게 노이즈에 과적합(Overfitting)된 Latent Space는, 정작 중요한 '의미론적 정보(Semantic Info)'를 잃어버립니다. 다운스트림 Diffusion 모델이 이 Latent를 보고 학습해야 하는데, VAE가 쓸데없이 노이즈까지 꽉꽉 채워 넣은 정보를 던져주니 생성이 제대로 될 리가 없죠.

실제로 최신 연구들(Yao et al., 2025)을 보면 VAE의 재구성 수치(rFID)가 좋아졌는데, 최종 생성 모델의 성능(gFID)은 오히려 떨어지는 현상이 보고되고 있습니다.

## 마치며: 엔지니어의 시선

Hacker News의 한 유저는 "데이터 품질이 나빠서 재구성이 안 되는 걸 모델 탓으로 돌리고 있었다는 걸 깨달았다"고 댓글을 남겼습니다. 저 역시 뜨끔했습니다. 우리는 종종 나쁜 데이터를 거르는(Filtering) 대신, 모델 아키텍처를 비틀어서 그 나쁜 데이터까지 학습시키려 애씁니다.

앞으로의 트렌드는 명확해 보입니다. 픽셀 단위의 복원보다는 DINO나 CLIP 같은 모델을 활용해 **의미론적 정렬(Alignment)** 을 강화하는 방향으로 가거나, 아예 VAE 없이 픽셀 공간에서 학습하는 JIT(Just-In-Time) 같은 접근이 늘어날 것입니다.

**요약하자면:**
- VAE 학습에서 Group Norm은 양날의 검이다.
- 이미지와 비디오를 섞어 쓸 땐 Loss Scaling에 목숨 걸어야 한다.
- **가장 중요한 것:** 픽셀을 완벽하게 복원하려 하지 마라. 그건 노이즈를 외우는 짓이다.

4개월간의 삽질 끝에 얻은 이 교훈은, 지금 멀티모달 모델을 깎고 있는 우리 모두에게 값진 이정표가 될 것입니다.
