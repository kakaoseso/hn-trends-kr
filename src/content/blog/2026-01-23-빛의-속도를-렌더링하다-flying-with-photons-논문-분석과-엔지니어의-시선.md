---
title: "빛의 속도를 렌더링하다: Flying with Photons 논문 분석과 엔지니어의 시선"
description: "빛의 속도를 렌더링하다: Flying with Photons 논문 분석과 엔지니어의 시선"
pubDate: "2026-01-23T19:49:00Z"
---

솔직히 고백하자면, 처음 논문 제목을 봤을 때 "또 그럴듯한 제목을 붙인 3D 렌더링 데모겠지"라고 생각했습니다. 요즘 **NeRF** 나 **Gaussian Splatting** 관련 논문들이 쏟아져 나오면서 피로감을 느끼던 참이었거든요. 하지만 이 논문, *Flying with Photons*는 다릅니다. 이건 단순히 예쁜 이미지를 만드는 게 아니라, **빛의 전파(Light Propagation)** 라는 물리적 현상 자체를 딥러닝 파이프라인 안으로 끌어들였습니다.

오늘은 ECCV 2024에 발표된 이 흥미로운 연구를 엔지니어의 관점에서 뜯어보고, Hacker News(HN)의 반응과 제 생각을 섞어 정리해 보겠습니다.

## 1. 펨토 포토그래피의 귀환, 그리고 진화

혹시 10여 년 전, Ramesh Raskar 교수의 TED 강연을 기억하시나요? 콜라병을 통과하는 빛의 움직임을 초고속 카메라(Femto-photography)로 촬영한 그 전설적인 영상 말입니다. 당시에는 그저 "촬영"하는 것에 그쳤지만, 이 논문은 한 단계 더 나아갑니다.

- **Novel View Synthesis:** 촬영된 데이터(Ground Truth)를 바탕으로, 카메라가 없는 새로운 각도에서 빛이 이동하는 모습을 생성해 냅니다.
- **Transient Field:** 기존의 정적인 NeRF와 달리, 시간 축을 포함한 고차원 필드를 학습합니다.

핵심은 **3ns(나노초)**, 즉 우리가 눈을 깜빡이는 시간보다 1억 배 빠른 찰나의 순간을 렌더링한다는 점입니다. 단순히 비디오를 재생하는 게 아니라, 빛이 산란(Scattering)되고, 반사(Reflection)되고, 회절(Diffraction)되는 과정을 물리적으로 타당하게 그려냅니다.

## 2. 기술적 핵심: Transient Field와 상대성 이론

이 논문이 기술적으로 섹시한 이유는 **빛의 속도(Speed of Light)** 를 모델링에 명시적으로 포함했기 때문입니다.

일반적인 NeRF는 `(x, y, z, direction)`을 입력받아 `(RGB, Density)`를 뱉어냅니다. 하지만 여기서는 **Transient Field** 라는 개념을 도입합니다. 쉽게 말해, 3D 공간의 한 점과 방향이 주어졌을 때, 시간에 따라 변하는 **Radiance(빛의 세기)** 를 이산적인 시간 신호(Discrete-time signal)로 매핑합니다.

여기서 엔지니어로서 무릎을 탁 쳤던 부분은 **Time Delay** 처리 방식입니다.

- **문제:** 카메라와 피사체 사이의 거리가 멀어질수록, 빛이 도달하는 데 걸리는 시간이 달라집니다. 소위 말하는 상대론적 효과죠.
- **해결:** 신경망이 이 지연 시간을 다 외우게(Memorize) 하면 효율이 떨어집니다. 그래서 연구진은 `speed-of-light time delay`를 명시적으로 계산해서 Time Shift를 시킨 후 학습시켰습니다.

이 덕분에 네트워크는 "빛이 언제 도착하는지"를 학습하는 게 아니라, "해당 위치에서 빛의 파동이 어떻게 생겼는지"에 집중할 수 있게 됩니다. 엔지니어링 관점에서 보면 도메인 지식(물리학)을 활용해 모델의 복잡도를 획기적으로 낮춘 훌륭한 **Inductive Bias** 설계입니다.

## 3. Hacker News의 논쟁: 시뮬레이션 vs 측정

이 논문이 HN에 올라왔을 때, 댓글 창은 꽤나 뜨거웠습니다. 특히 한 유저의 지적이 눈에 띄더군요.

> "왜 굳이 이걸 측정해서 렌더링하죠? FDTD(Finite Difference Time Domain) 같은 완벽한 시뮬레이션 기법들이 이미 있잖아요. 그게 더 정확하지 않나요?"

매우 타당한 지적입니다. 물리 엔진 잘 돌리면 빛의 경로야 수학적으로 완벽하게 계산할 수 있으니까요. 하지만 저는 다른 유저의 반박에 더 공감합니다.

- **Measurement is Reality:** 시뮬레이션은 우리가 정의한 조건(가정) 안에서만 완벽합니다. 실제 콜라병 유리의 미세한 기포, 공기 중의 먼지, 재질의 불균일함은 시뮬레이션하기 너무 어렵습니다.
- **Data-Driven:** 이 논문은 실제 데이터를 측정(Measurement)하고, 그 측정값 사이의 빈 공간을 신경망으로 채웁니다. 즉, **Reality** 를 기반으로 한 보간(Interpolation)입니다.

"Real"이 무엇이냐에 대한 철학적 논쟁도 있었지만, 엔지니어 입장에서 보자면 **Simulation** 은 설계 검증용이고, 이런 **Neural Rendering** 은 현실 세계의 Digital Twin을 구현하는 데 필수적인 기술입니다.

## 4. 개인적인 평가와 전망

이 기술, 당장 상용화해서 어디에 쓸 수 있을까요? 솔직히 게임이나 일반적인 VR/AR에 쓰기엔 오버스펙입니다. 누가 게임에서 빛이 3나노초 동안 이동하는 걸 보고 싶어 하겠습니까?

하지만 **Scientific Visualization** 이나 **Inspection** 분야에서는 이야기가 다릅니다.

1.  **비파괴 검사:** 반투명한 물체 내부에서 빛이 산란되는 패턴을 분석해 결함을 찾아내는 데 응용될 수 있습니다.
2.  **자율주행 센서 검증:** LiDAR나 ToF(Time of Flight) 센서가 극한의 환경(안개, 비)에서 어떻게 반응할지 시뮬레이션이 아닌 실측 기반으로 렌더링해 볼 수 있습니다.

**Verdict:**
이 논문은 NeRF 계열 연구가 단순히 "사진 같은 3D"를 넘어, 인간의 눈으로 볼 수 없는 **초고속 물리 현상** 까지 영역을 확장했다는 데 큰 의의가 있습니다. 코드를 당장 프로덕션에 넣을 순 없겠지만, **Inverse Rendering** 을 통해 물체의 숨겨진 물성을 파악하려는 시도는 앞으로 계속될 겁니다.

엔지니어로서 이런 "미친(Crazy)" 연구들을 볼 때마다 가슴이 뜁니다. 결국 우리가 다루는 데이터의 한계는 센서가 아니라, 그 데이터를 해석하는 알고리즘에 달려 있다는 걸 증명하니까요.
